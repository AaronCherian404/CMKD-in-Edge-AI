# CMKD-in-Edge-AI

While edge AI continues to offer promising solutions for real-time applications, the
deployment of multi-modal deep learning models remains particularly challenging
considering their substantial computational and memory requirements on resourceconstrained devices. This paper approaches this problem through the integration of a
Resource-Aware Cross-Modal Knowledge Distillation (CMKD). Our approach introduces
three key checkmarks: (1) a dynamic resource allocation mechanism that adaptively
manages computational resources across different modalities based on real-time
device constraints, (2) modality-specific compression techniques that optimize
knowledge transfer while minimizing memory footprint, and (3) a lightweight feature
alignment strategy that maintains cross-modal performance under varying resource
conditions
